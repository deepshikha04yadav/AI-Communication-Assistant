{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepshikha04yadav/AI-Communication-Assistant/blob/main/Welcome_to_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "pyvNPlyAW0Gv",
        "outputId": "e6dae790-ffe8-4efa-f468-97193bedc2e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers==0.21.0 transformers==4.30.2 accelerate==0.20.3 safetensors==0.3.1 xformers==0.0.20 Pillow==9.5.0 numpy==1.24.4 matplotlib==3.7.2 gradio==4.0.0"
      ],
      "metadata": {
        "id": "hgZvaNhbWz9J",
        "outputId": "5624c44b-ea22-4b0a-b838-ba8ddf30660c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting diffusers==0.21.0\n",
            "  Downloading diffusers-0.21.0.tar.gz (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers==4.30.2\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.20.3\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting safetensors==0.3.1\n",
            "  Downloading safetensors-0.3.1.tar.gz (34 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting xformers==0.0.20\n",
            "  Downloading xformers-0.0.20.tar.gz (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Pillow==9.5.0\n",
            "  Downloading Pillow-9.5.0.tar.gz (50.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4.tar.gz (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install safetensors==0.3.1 xformers==0.0.20 Pillow==9.5.0"
      ],
      "metadata": {
        "id": "V8CQXv4kb7HF",
        "outputId": "a8e0cbce-18a2-4be2-c1a2-bbf5301a67d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting safetensors==0.3.1\n",
            "  Using cached safetensors-0.3.1.tar.gz (34 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting xformers==0.0.20\n",
            "  Using cached xformers-0.0.20.tar.gz (7.6 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Pillow==9.5.0\n",
            "  Using cached Pillow-9.5.0.tar.gz (50.5 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.12 in /usr/local/lib/python3.12/dist-packages (from xformers==0.0.20) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xformers==0.0.20) (2.0.2)\n",
            "Collecting pyre-extensions==0.0.29 (from xformers==0.0.20)\n",
            "  Downloading pyre_extensions-0.0.29-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting typing-inspect (from pyre-extensions==0.0.29->xformers==0.0.20)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pyre-extensions==0.0.29->xformers==0.0.20) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->xformers==0.0.20) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.12->xformers==0.0.20) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.12->xformers==0.0.20) (3.0.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers==0.0.20)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: safetensors, xformers, Pillow\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for safetensors \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for safetensors (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for safetensors\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "\n",
            "ok\n",
            "ok\n",
            "  Building wheel for xformers (setup.py) ... \u001b[?25l\u001b[?25hcanceled\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.24.4 matplotlib==3.7.2 gradio==4.0.0"
      ],
      "metadata": {
        "id": "0lRGLgRtWz5h",
        "outputId": "80c29005-48cf-4a93-f4b2-39e75bfbd3d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.24.4\n",
            "  Using cached numpy-1.24.4.tar.gz (10.9 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "-5l148waWz2P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import autocast\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "from typing import Optional, Tuple, List\n",
        "from datetime import datetime\n",
        "from importlib.metadata import version\n",
        "\n",
        "from diffusers import (\n",
        "    StableDiffusionPipeline,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    DPMSolverMultistepScheduler,\n",
        "    DDIMScheduler,\n",
        "    LMSDiscreteScheduler\n",
        ")\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "TJ81utMgdRZE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Pytorch version: {torch.__version__}')\n",
        "print(f'CUDA version: {torch.cuda.is_available}')\n",
        "print(f'GPU device:{torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\"}')"
      ],
      "metadata": {
        "id": "lha2jjO6e9ng",
        "outputId": "fc0f7989-3590-4ada-b3fa-88fbcee3b3cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytorch version: 2.8.0+cu126\n",
            "CUDA version: <function is_available at 0x7c99a85b44a0>\n",
            "GPU device:Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core Stable Diffusion Generator class\n",
        "class StableDiffusionGenerator:\n",
        "    def __init__(self, model_id: str = \"runwayml/stable-diffusion-v1-5\", device: str = \"auto\"):\n",
        "        try:\n",
        "            self.device = self._setup_device(device)\n",
        "            self.dtype = torch.float16 if self.device.type == \"cuda\" else torch.float32\n",
        "\n",
        "            print(f\"🚀 Initializing Stable Diffusion on {self.device}\")\n",
        "            print(f\"📊 Using precision: {self.dtype}\")\n",
        "\n",
        "            torch_version = version(\"torch\")\n",
        "            diffusers_version = version(\"diffusers\")\n",
        "            print(f\"📦 PyTorch version: {torch_version}\")\n",
        "            print(f\"📦 Diffusers version: {diffusers_version}\")\n",
        "\n",
        "            self.pipe = self._load_pipeline(model_id)\n",
        "            self.current_scheduler = \"euler_a\"\n",
        "            self.schedulers = {\n",
        "                \"euler_a\": (\"Euler Ancestral\", \"Fast, good for creative images\"),\n",
        "                \"euler\": (\"Euler\", \"Deterministic, consistent results\"),\n",
        "                \"ddim\": (\"DDIM\", \"Classic, good quality, slower\"),\n",
        "                \"dpm_solver\": (\"DPM Solver\", \"High quality, efficient\"),\n",
        "                \"lms\": (\"LMS\", \"Linear multistep, stable\")\n",
        "            }\n",
        "            print(\"✅ Stable Diffusion Generator Ready!\")\n",
        "            print(f\"📝 Available Schedulers: {list(self.schedulers.keys())}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Initialization Error: {str(e)}\")\n",
        "            print(\"Please ensure Visual C++ Redistributable 2015-2022 is installed\")\n",
        "            raise\n",
        "\n",
        "    def _setup_device(self, device: str) -> torch.device:\n",
        "        if device == \"auto\":\n",
        "            if torch.cuda.is_available():\n",
        "                device = \"cuda\"\n",
        "                print(f\"🎯 GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
        "                print(f\"💾 VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "            else:\n",
        "                device = \"cpu\"\n",
        "                print(\"💻 Using CPU (GPU not available)\")\n",
        "        return torch.device(device)\n",
        "\n",
        "    def _load_pipeline(self, model_id: str) -> StableDiffusionPipeline:\n",
        "        try:\n",
        "            pipe = StableDiffusionPipeline.from_pretrained(\n",
        "                model_id,\n",
        "                torch_dtype=self.dtype,\n",
        "                safety_checker=None,\n",
        "                requires_safety_checker=False,\n",
        "            )\n",
        "            print(\"🔧 Applying Memory Optimizations...\")\n",
        "            pipe.enable_attention_slicing()\n",
        "            print(\"  ✓ Attention Slicing: Enabled\")\n",
        "            pipe.enable_vae_slicing()\n",
        "            print(\"  ✓ VAE Slicing: Enabled\")\n",
        "            try:\n",
        "                pipe.enable_xformers_memory_efficient_attention()\n",
        "                print(\"  ✓ XFormers Attention: Enabled\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠ XFormers: Not available ({e})\")\n",
        "            if self.device.type == \"cuda\":\n",
        "                try:\n",
        "                    pipe = pipe.to(self.device)\n",
        "                    print(\"  ✓ Full GPU Loading: Success\")\n",
        "                except RuntimeError as e:\n",
        "                    print(\"  ⚠ GPU Memory Limited: Using CPU Offload\")\n",
        "                    pipe.enable_model_cpu_offload()\n",
        "            else:\n",
        "                pipe.enable_sequential_cpu_offload()\n",
        "                print(\"  ✓ CPU Sequential Offload: Enabled\")\n",
        "            return pipe\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load model: {e}\")\n",
        "\n",
        "    def set_scheduler(self, scheduler_name: str) -> bool:\n",
        "        if scheduler_name not in self.schedulers:\n",
        "            print(f\"❌ Unknown scheduler: {scheduler_name}\")\n",
        "            return False\n",
        "        if scheduler_name == self.current_scheduler:\n",
        "            return True\n",
        "        scheduler_map = {\n",
        "            \"euler_a\": EulerAncestralDiscreteScheduler,\n",
        "            \"euler\": EulerDiscreteScheduler,\n",
        "            \"ddim\": DDIMScheduler,\n",
        "            \"dpm_solver\": DPMSolverMultistepScheduler,\n",
        "            \"lms\": LMSDiscreteScheduler\n",
        "        }\n",
        "        try:\n",
        "            scheduler_class = scheduler_map[scheduler_name]\n",
        "            self.pipe.scheduler = scheduler_class.from_config(self.pipe.scheduler.config)\n",
        "            self.current_scheduler = scheduler_name\n",
        "            name, desc = self.schedulers[scheduler_name]\n",
        "            print(f\"🔄 Scheduler Changed: {name} ({desc})\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Scheduler Error: {e}\")\n",
        "            return False\n",
        "\n",
        "    def generate_image(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        negative_prompt: str = \"\",\n",
        "        width: int = 512,\n",
        "        height: int = 512,\n",
        "        num_inference_steps: int = 20,\n",
        "        guidance_scale: float = 7.5,\n",
        "        seed: Optional[int] = None,\n",
        "        scheduler: str = \"euler_a\"\n",
        "    ) -> Tuple[Image.Image, dict]:\n",
        "        if not prompt.strip():\n",
        "            raise ValueError(\"Prompt cannot be empty\")\n",
        "        self.set_scheduler(scheduler)\n",
        "        if seed is None:\n",
        "            seed = torch.randint(0, 2**32, (1,)).item()\n",
        "        generator = torch.Generator(device=self.device)\n",
        "        generator.manual_seed(seed)\n",
        "        width = (width // 8) * 8\n",
        "        height = (height // 8) * 8\n",
        "        print(f\"🎨 Generating: '{prompt[:50]}...'\")\n",
        "        print(f\"📏 Size: {width}x{height}, Steps: {num_inference_steps}, CFG: {guidance_scale}\")\n",
        "        print(f\"🎲 Seed: {seed}, Scheduler: {scheduler}\")\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            with torch.inference_mode():\n",
        "                if self.device.type == \"cuda\" and self.dtype == torch.float16:\n",
        "                    with autocast(self.device.type):\n",
        "                        result = self.pipe(\n",
        "                            prompt=prompt,\n",
        "                            negative_prompt=negative_prompt if negative_prompt else None,\n",
        "                            width=width,\n",
        "                            height=height,\n",
        "                            num_inference_steps=num_inference_steps,\n",
        "                            guidance_scale=guidance_scale,\n",
        "                            generator=generator\n",
        "                        )\n",
        "                else:\n",
        "                    result = self.pipe(\n",
        "                        prompt=prompt,\n",
        "                        negative_prompt=negative_prompt if negative_prompt else None,\n",
        "                        width=width,\n",
        "                        height=height,\n",
        "                        num_inference_steps=num_inference_steps,\n",
        "                        guidance_scale=guidance_scale,\n",
        "                        generator=generator\n",
        "                    )\n",
        "            generation_time = time.time() - start_time\n",
        "            metadata = {\n",
        "                \"prompt\": prompt,\n",
        "                \"negative_prompt\": negative_prompt,\n",
        "                \"width\": width,\n",
        "                \"height\": height,\n",
        "                \"steps\": num_inference_steps,\n",
        "                \"guidance_scale\": guidance_scale,\n",
        "                \"scheduler\": scheduler,\n",
        "                \"seed\": seed,\n",
        "                \"generation_time\": round(generation_time, 2),\n",
        "                \"device\": str(self.device),\n",
        "                \"dtype\": str(self.dtype)\n",
        "            }\n",
        "            print(f\"✅ Generated in {generation_time:.2f}s\")\n",
        "            return result.images[0], metadata\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            self._cleanup_memory()\n",
        "            raise RuntimeError(\n",
        "                \"GPU Out of Memory! Try: reducing image size, fewer steps, \"\n",
        "                \"or use CPU mode. Current settings may be too demanding.\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Generation failed: {str(e)}\")\n",
        "        finally:\n",
        "            self._cleanup_memory()\n",
        "\n",
        "    def _cleanup_memory(self):\n",
        "        gc.collect()\n",
        "        if self.device.type == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def get_memory_usage(self) -> dict:\n",
        "        memory_info = {}\n",
        "        if self.device.type == \"cuda\":\n",
        "            memory_info = {\n",
        "                \"allocated_gb\": torch.cuda.memory_allocated() / 1024**3,\n",
        "                \"reserved_gb\": torch.cuda.memory_reserved() / 1024**3,\n",
        "                \"max_allocated_gb\": torch.cuda.max_memory_allocated() / 1024**3,\n",
        "                \"total_gb\": torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "            }\n",
        "        else:\n",
        "            memory_info = {\"device\": \"cpu\", \"note\": \"CPU memory tracking not available\"}\n",
        "        return memory_info\n",
        "\n",
        "    def save_image(self, image: Image.Image, metadata: dict, output_dir: str = \"outputs\") -> str:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"sd_gen_{timestamp}_s{metadata['seed']}_{metadata['width']}x{metadata['height']}.png\"\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "        image.save(filepath)\n",
        "        metadata_file = filepath.replace('.png', '_metadata.txt')\n",
        "        with open(metadata_file, 'w') as f:\n",
        "            f.write(\"Stable Diffusion Generation Metadata\\n\")\n",
        "            f.write(\"=\" * 40 + \"\\n\")\n",
        "            for key, value in metadata.items():\n",
        "                f.write(f\"{key}: {value}\\n\")\n",
        "        print(f\"💾 Saved: {filepath}\")\n",
        "        return filepath"
      ],
      "metadata": {
        "id": "F1IodZ_1dcdV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio UI class for Stable Diffusion\n",
        "class StableDiffusionUI:\n",
        "    def __init__(self):\n",
        "        self.generator = None\n",
        "        self.gallery_images = []\n",
        "        self.generation_history = []\n",
        "\n",
        "    def initialize_generator(self, model_choice: str, device_choice: str) -> str:\n",
        "        try:\n",
        "            model_map = {\n",
        "                \"Stable Diffusion 1.5 (Recommended)\": \"runwayml/stable-diffusion-v1-5\",\n",
        "                \"Stable Diffusion 2.1\": \"stabilityai/stable-diffusion-2-1\",\n",
        "                \"Realistic Vision (RealVisXL)\": \"SG161222/RealVisXL_V4.0\"\n",
        "            }\n",
        "            device_map = {\n",
        "                \"Auto (Recommended)\": \"auto\",\n",
        "                \"GPU (CUDA)\": \"cuda\",\n",
        "                \"CPU (Slower)\": \"cpu\"\n",
        "            }\n",
        "            model_id = model_map.get(model_choice, \"runwayml/stable-diffusion-v1-5\")\n",
        "            device = device_map.get(device_choice, \"auto\")\n",
        "            self.generator = StableDiffusionGenerator(model_id=model_id, device=device)\n",
        "            memory_info = self.generator.get_memory_usage()\n",
        "            memory_text = f\"Memory Usage: {memory_info}\" if memory_info else \"Ready!\"\n",
        "            return f\"✅ Model loaded successfully!\\n{memory_text}\"\n",
        "        except Exception as e:\n",
        "            return f\"❌ Initialization failed: {str(e)}\"\n",
        "\n",
        "    def generate_image(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        negative_prompt: str,\n",
        "        width: int,\n",
        "        height: int,\n",
        "        steps: int,\n",
        "        guidance: float,\n",
        "        scheduler: str,\n",
        "        seed: int,\n",
        "        save_image: bool\n",
        "    ) -> Tuple[Optional[Image.Image], str, str]:\n",
        "        if self.generator is None:\n",
        "            return None, \"❌ Please initialize the model first!\", \"\"\n",
        "        if not prompt.strip():\n",
        "            return None, \"❌ Please enter a prompt!\", \"\"\n",
        "        try:\n",
        "            seed = None if seed == -1 else int(seed)\n",
        "            image, metadata = self.generator.generate_image(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=negative_prompt,\n",
        "                width=width,\n",
        "                height=height,\n",
        "                num_inference_steps=steps,\n",
        "                guidance_scale=guidance,\n",
        "                scheduler=scheduler,\n",
        "                seed=seed\n",
        "            )\n",
        "            info_text = self._format_generation_info(metadata)\n",
        "            saved_path = \"\"\n",
        "            if save_image:\n",
        "                saved_path = self.generator.save_image(image, metadata)\n",
        "            self.generation_history.append(metadata)\n",
        "            self.gallery_images.append(image)\n",
        "            if len(self.gallery_images) > 10:\n",
        "                self.gallery_images = self.gallery_images[-10:]\n",
        "                self.generation_history = self.generation_history[-10:]\n",
        "            return image, info_text, saved_path\n",
        "        except Exception as e:\n",
        "            return None, f\"❌ Generation failed: {str(e)}\", \"\"\n",
        "\n",
        "    def _format_generation_info(self, metadata: dict) -> str:\n",
        "        return f\"\"\"\n",
        "✅ Generation Complete!\n",
        "\n",
        "🎯 **Parameters Used:**\n",
        "• Prompt: {metadata['prompt'][:100]}{'...' if len(metadata['prompt']) > 100 else ''}\n",
        "• Size: {metadata['width']} × {metadata['height']} pixels\n",
        "• Steps: {metadata['steps']} (more steps = higher quality, slower)\n",
        "• Guidance Scale: {metadata['guidance_scale']} (higher = follows prompt more closely)\n",
        "• Scheduler: {metadata['scheduler']}\n",
        "• Seed: {metadata['seed']} (for reproducible results)\n",
        "\n",
        "⏱️ **Performance:**\n",
        "• Generation Time: {metadata['generation_time']}s\n",
        "• Device: {metadata['device']}\n",
        "• Precision: {metadata['dtype']}\n",
        "\"\"\"\n",
        "\n",
        "    def get_example_prompts(self) -> list:\n",
        "        return [\n",
        "            [\"a serene mountain landscape at sunrise, photorealistic, highly detailed\", \"blurry, low quality\"],\n",
        "            [\"portrait of a wise old wizard, fantasy art, digital painting\", \"ugly, deformed\"],\n",
        "            [\"cyberpunk cityscape at night, neon lights, futuristic\", \"daytime, bright\"],\n",
        "            [\"cute cartoon cat wearing a hat, kawaii style\", \"realistic, scary\"],\n",
        "            [\"abstract geometric patterns, colorful, modern art\", \"representational, dull colors\"]\n",
        "        ]\n",
        "\n",
        "    def show_scheduler_info(self, scheduler: str) -> str:\n",
        "        scheduler_info = {\n",
        "            \"euler_a\": \"**Euler Ancestral**: Fast and creative, adds slight randomness for variety\",\n",
        "            \"euler\": \"**Euler**: Deterministic and consistent, same seed = same result\",\n",
        "            \"ddim\": \"**DDIM**: Classic scheduler, high quality but slower\",\n",
        "            \"dpm_solver\": \"**DPM Solver**: Efficient high-quality generation\",\n",
        "            \"lms\": \"**LMS**: Linear multistep, very stable results\"\n",
        "        }\n",
        "        return scheduler_info.get(scheduler, \"Scheduler information not available\")\n",
        "\n",
        "    def get_memory_info(self) -> str:\n",
        "        if self.generator is None:\n",
        "            return \"Model not loaded\"\n",
        "        try:\n",
        "            memory_info = self.generator.get_memory_usage()\n",
        "            if 'allocated_gb' in memory_info:\n",
        "                return f\"\"\"\n",
        "GPU Memory Usage:\n",
        "• Allocated: {memory_info['allocated_gb']:.2f}GB\n",
        "• Reserved: {memory_info['reserved_gb']:.2f}GB\n",
        "• Total Available: {memory_info['total_gb']:.2f}GB\n",
        "• Usage: {(memory_info['allocated_gb']/memory_info['total_gb']*100):.1f}%\n",
        "                \"\"\"\n",
        "            else:\n",
        "                return \"CPU mode - memory tracking not available\"\n",
        "        except:\n",
        "            return \"Memory info unavailable\"\n",
        "\n",
        "    def create_interface(self) -> gr.Blocks:\n",
        "        with gr.Blocks(\n",
        "            title=\"🎨 Educational Stable Diffusion Generator\",\n",
        "            theme=gr.themes.Soft()\n",
        "        ) as interface:\n",
        "            gr.Markdown(\"\"\"\n",
        "            # 🎨 Educational Stable Diffusion Text-to-Image Generator\n",
        "            **Learn Generative AI concepts while creating images!**\n",
        "            \"\"\")\n",
        "            with gr.Tab(\"🚀 Setup & Generation\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        gr.Markdown(\"### 🔧 Model Setup\")\n",
        "                        model_choice = gr.Dropdown(\n",
        "                            choices=[\n",
        "                                \"Stable Diffusion 1.5 (Recommended)\",\n",
        "                                \"Stable Diffusion 2.1\",\n",
        "                                \"Realistic Vision (RealVisXL)\"\n",
        "                            ],\n",
        "                            value=\"Stable Diffusion 1.5 (Recommended)\",\n",
        "                            label=\"Model Selection\"\n",
        "                        )\n",
        "                        device_choice = gr.Dropdown(\n",
        "                            choices=[\n",
        "                                \"Auto (Recommended)\",\n",
        "                                \"GPU (CUDA)\",\n",
        "                                \"CPU (Slower)\"\n",
        "                            ],\n",
        "                            value=\"Auto (Recommended)\",\n",
        "                            label=\"Device Selection\"\n",
        "                        )\n",
        "                        init_btn = gr.Button(\"🚀 Initialize Model\", variant=\"primary\")\n",
        "                        init_status = gr.Textbox(\n",
        "                            label=\"Initialization Status\",\n",
        "                            placeholder=\"Click Initialize Model to start\",\n",
        "                            lines=3\n",
        "                        )\n",
        "                    with gr.Column():\n",
        "                        gr.Markdown(\"### 📊 System Info\")\n",
        "                        memory_btn = gr.Button(\"📊 Check Memory Usage\")\n",
        "                        memory_info = gr.Textbox(\n",
        "                            label=\"Memory Information\",\n",
        "                            placeholder=\"Click to check memory usage\",\n",
        "                            lines=6\n",
        "                        )\n",
        "                gr.Markdown(\"### ✨ Image Generation\")\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        prompt = gr.Textbox(\n",
        "                            label=\"🎯 Prompt (Describe what you want)\",\n",
        "                            placeholder=\"a beautiful landscape painting, oil on canvas, detailed\",\n",
        "                            lines=3\n",
        "                        )\n",
        "                        negative_prompt = gr.Textbox(\n",
        "                            label=\"🚫 Negative Prompt (What to avoid)\",\n",
        "                            placeholder=\"blurry, low quality, bad anatomy\",\n",
        "                            lines=2\n",
        "                        )\n",
        "                        generate_btn = gr.Button(\"🎨 Generate Image\", variant=\"primary\", size=\"lg\")\n",
        "                    with gr.Column():\n",
        "                        with gr.Accordion(\"🔧 Advanced Settings\", open=True):\n",
        "                            with gr.Row():\n",
        "                                width = gr.Slider(256, 1024, 512, step=64, label=\"Width\")\n",
        "                                height = gr.Slider(256, 1024, 512, step=64, label=\"Height\")\n",
        "                            with gr.Row():\n",
        "                                steps = gr.Slider(10, 100, 20, step=1, label=\"Inference Steps\")\n",
        "                                guidance = gr.Slider(1.0, 20.0, 7.5, step=0.5, label=\"Guidance Scale\")\n",
        "                            scheduler = gr.Dropdown(\n",
        "                                choices=[\"euler_a\", \"euler\", \"ddim\", \"dpm_solver\", \"lms\"],\n",
        "                                value=\"euler_a\",\n",
        "                                label=\"Scheduler\"\n",
        "                            )\n",
        "                            scheduler_info = gr.Textbox(\n",
        "                                label=\"Scheduler Information\",\n",
        "                                interactive=False,\n",
        "                                lines=2\n",
        "                            )\n",
        "                            with gr.Row():\n",
        "                                seed = gr.Number(-1, label=\"Seed\")\n",
        "                                save_image = gr.Checkbox(True, label=\"💾 Save Generated Images\")\n",
        "                with gr.Row():\n",
        "                    output_image = gr.Image(label=\"🖼️ Generated Image\", type=\"pil\")\n",
        "                with gr.Row():\n",
        "                    generation_info = gr.Textbox(\n",
        "                        label=\"📝 Generation Information\",\n",
        "                        lines=10,\n",
        "                        interactive=False\n",
        "                    )\n",
        "                    saved_path = gr.Textbox(\n",
        "                        label=\"💾 Saved File Path\",\n",
        "                        interactive=False\n",
        "                    )\n",
        "            with gr.Tab(\"📚 Learning Resources\"):\n",
        "                gr.Markdown(\"\"\"\n",
        "                ## 🧠 Understanding Stable Diffusion\n",
        "                ### What is Diffusion?\n",
        "                Diffusion models learn to gradually remove noise from random data.\n",
        "                ### Key Components:\n",
        "                **🎯 CLIP (Text Encoder)**\n",
        "                **🧮 U-Net (Denoising Network)**\n",
        "                **🎨 VAE (Variational Autoencoder)**\n",
        "                **⚙️ Schedulers**\n",
        "                ### Parameter Guide:\n",
        "                **Steps (10-100)**: More steps = higher quality but slower generation\n",
        "                **Guidance Scale (1-20)**: Higher values make the AI follow your prompt more strictly\n",
        "                **Seed**: Controls randomness - same seed + settings = same image\n",
        "                **Resolution**: Higher resolution = more detail but needs more GPU memory\n",
        "                \"\"\")\n",
        "            with gr.Tab(\"🖼️ Examples & Gallery\"):\n",
        "                gr.Markdown(\"### 🎨 Example Prompts to Try\")\n",
        "                examples = gr.Examples(\n",
        "                    examples=self.get_example_prompts(),\n",
        "                    inputs=[prompt, negative_prompt],\n",
        "                    label=\"Click any example to load it\"\n",
        "                )\n",
        "                gr.Markdown(\"### 🖼️ Recent Generations\")\n",
        "                gallery = gr.Gallery(\n",
        "                    value=[],\n",
        "                    label=\"Your Generated Images\",\n",
        "                    show_label=True,\n",
        "                    elem_id=\"gallery\",\n",
        "                    columns=3,\n",
        "                    rows=2,\n",
        "                    object_fit=\"contain\",\n",
        "                    height=\"auto\"\n",
        "                )\n",
        "            # Event handlers\n",
        "            init_btn.click(\n",
        "                fn=self.initialize_generator,\n",
        "                inputs=[model_choice, device_choice],\n",
        "                outputs=init_status\n",
        "            )\n",
        "            generate_btn.click(\n",
        "                fn=self.generate_image,\n",
        "                inputs=[prompt, negative_prompt, width, height, steps, guidance, scheduler, seed, save_image],\n",
        "                outputs=[output_image, generation_info, saved_path]\n",
        "            ).then(\n",
        "                fn=lambda: self.gallery_images,\n",
        "                outputs=gallery\n",
        "            )\n",
        "            scheduler.change(\n",
        "                fn=self.show_scheduler_info,\n",
        "                inputs=scheduler,\n",
        "                outputs=scheduler_info\n",
        "            )\n",
        "            memory_btn.click(\n",
        "                fn=self.get_memory_info,\n",
        "                outputs=memory_info\n",
        "            )\n",
        "        return interface"
      ],
      "metadata": {
        "id": "baBRxg52dy6q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the Gradio interface\n",
        "ui = StableDiffusionUI()\n",
        "interface = ui.create_interface()\n",
        "interface.launch(\n",
        "    share=True,  # Set to True for public sharing\n",
        "    server_name=\"0.0.0.0\",\n",
        "    server_port=7860,\n",
        "    debug=False,\n",
        "    show_error=True\n",
        ")"
      ],
      "metadata": {
        "id": "rxeiRxAWdy2R",
        "outputId": "15bd4cd7-a2bd-4024-f818-9c835f6f7e41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a82b7b810abbbb7bb7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a82b7b810abbbb7bb7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aRvmnOd9dyyB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}